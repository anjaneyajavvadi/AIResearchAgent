{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37b2807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf85f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b6015-1c4c-71e0-89ec-82914f3dc33d-0', usage_metadata={'input_tokens': 2, 'output_tokens': 31, 'total_tokens': 33, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 22}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2e5dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict,List,Dict,Optional,Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34ad0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetryState(TypedDict):\n",
    "    rag:int\n",
    "    web_search:int\n",
    "    synthesis:int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b60d49fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalResult(TypedDict):\n",
    "    source: Literal[\"rag\", \"web\"]\n",
    "    title: str\n",
    "    content: str\n",
    "    url: Optional[str]\n",
    "    score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1db98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict,List,Dict,Optional,Literal\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    user_query:str\n",
    "\n",
    "    confidence_score:float\n",
    "    retrieval_mode:Literal['rag','web','both','none']\n",
    "    research_relevant:bool\n",
    "    answer_mode:Literal['grounded','direct','refuse']\n",
    "\n",
    "    rag_results:Optional[List[RetrievalResult]]\n",
    "    web_search_results:Optional[List[RetrievalResult]]\n",
    "    merged_results:Optional[List[RetrievalResult]]\n",
    "\n",
    "    retries:RetryState\n",
    "    max_retries:RetryState\n",
    "\n",
    "    failure_response:Optional[str]\n",
    "    response:Optional[str]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72bd82bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'planner_node' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StateGraph, END\n\u001b[0;32m      3\u001b[0m graph \u001b[38;5;241m=\u001b[39m StateGraph(AgentState)\n\u001b[1;32m----> 5\u001b[0m graph\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplanner\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mplanner_node\u001b[49m)\n\u001b[0;32m      6\u001b[0m graph\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag\u001b[39m\u001b[38;5;124m\"\u001b[39m, rag_node)\n\u001b[0;32m      7\u001b[0m graph\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search\u001b[39m\u001b[38;5;124m\"\u001b[39m, websearch_node)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'planner_node' is not defined"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"planner\", planner_node)\n",
    "graph.add_node(\"rag\", rag_node)\n",
    "graph.add_node(\"web_search\", websearch_node)\n",
    "graph.add_node(\"confidence_checker\", confidence_checker)\n",
    "graph.add_node(\"evaluator\", evaluator_node)\n",
    "graph.add_node(\"summarizer\", summarizer_node)\n",
    "graph.add_node(\"failure_node\", failure_node)\n",
    "\n",
    "graph.set_entry_point(\"planner\")\n",
    "\n",
    "# Planner decides retrieval\n",
    "graph.add_conditional_edges(\n",
    "    \"planner\",\n",
    "    planner_router,\n",
    "    {\n",
    "        \"rag\": \"rag\",\n",
    "        \"web\": \"web_search\",\n",
    "        \"both\": [\"rag\", \"web_search\"],\n",
    "        \"none\": \"failure_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Retrieval → confidence\n",
    "graph.add_edge(\"rag\", \"confidence_checker\")\n",
    "graph.add_edge(\"web_search\", \"confidence_checker\")\n",
    "\n",
    "# Confidence → evaluation\n",
    "graph.add_edge(\"confidence_checker\", \"evaluator\")\n",
    "\n",
    "# Evaluation controls loop\n",
    "graph.add_conditional_edges(\n",
    "    \"evaluator\",\n",
    "    evaluation_router,\n",
    "    {\n",
    "        \"planner\": \"planner\",\n",
    "        \"failed\": \"failure_node\",\n",
    "        \"enough\": \"summarizer\"\n",
    "    }\n",
    ")\n",
    "\n",
    "graph.add_edge(\"summarizer\", END)\n",
    "graph.add_edge(\"failure_node\", END)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc0fd8a",
   "metadata": {},
   "source": [
    "<h1>Planner Node</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff428de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PlannerOutput(research_relevant=False, retrieval_mode='none', answer_mode='refuse', confidence=1.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "\n",
    "class PlannerOutput(BaseModel):\n",
    "    research_relevant: bool\n",
    "    retrieval_mode: Literal[\"rag\", \"web\", \"both\", \"none\"]\n",
    "    answer_mode: Literal[\"grounded\",\"direct\",\"refuse\"]\n",
    "    confidence: float = Field(ge=0.0, le=1.0)\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=PlannerOutput)\n",
    "\n",
    "\n",
    "PLANNER_PROMPT = \"\"\"You are a planning module inside an AI research assistant.\n",
    "\n",
    "You DO NOT answer the user.\n",
    "You DO NOT explain your reasoning.\n",
    "You ONLY decide how the system should respond.\n",
    "\n",
    "You have access to:\n",
    "- A large internal vector store containing AI / ML / NLP / LLM / Systems research papers, surveys, benchmarks, and technical articles.\n",
    "- A web search tool for retrieving recent, external, or missing research information.\n",
    "\n",
    "Your job is to analyze the user query and output a structured decision describing:\n",
    "1. Whether the query is research-related\n",
    "2. How information should be retrieved (if at all)\n",
    "3. How the answer should be produced\n",
    "4. How confident you are in these decisions\n",
    "\n",
    "--------------------------------\n",
    "1. RESEARCH RELEVANCE\n",
    "--------------------------------\n",
    "A query IS research-related if it involves:\n",
    "- Research papers, surveys, or academic work\n",
    "- Models, algorithms, architectures, or methods\n",
    "- Benchmarks, evaluations, comparisons, or ablations\n",
    "- Technical analysis of AI/ML/NLP/LLMs/Systems\n",
    "\n",
    "A query is NOT research-related if it involves:\n",
    "- Casual conversation or chit-chat\n",
    "- Opinions, jokes, or storytelling\n",
    "- Personal advice or life questions\n",
    "- Shopping, travel, or entertainment\n",
    "- General trivia without technical depth\n",
    "\n",
    "Set:\n",
    "- research_relevant = true or false\n",
    "\n",
    "--------------------------------\n",
    "2. RETRIEVAL MODE\n",
    "--------------------------------\n",
    "If research_relevant = false:\n",
    "- retrieval_mode MUST be \"none\"\n",
    "\n",
    "If research_relevant = true, choose exactly ONE:\n",
    "\n",
    "Use \"rag\" if:\n",
    "- The topic is established or well-documented\n",
    "- Surveys, classic papers, or known methods are sufficient\n",
    "- Internal vector store is likely enough\n",
    "\n",
    "Use \"web\" if:\n",
    "- The query explicitly asks for recent, latest, or current work\n",
    "- The topic involves fast-moving developments\n",
    "- External or up-to-date sources are required\n",
    "\n",
    "Use \"both\" if:\n",
    "- Foundational research exists internally\n",
    "- AND recent updates, comparisons, or new papers may be required\n",
    "\n",
    "Rules:\n",
    "- Do NOT assume freshness unless explicitly requested\n",
    "- Prefer internal knowledge when possible\n",
    "- Be conservative: choose \"both\" only when clearly necessary\n",
    "- Never hallucinate missing information\n",
    "\n",
    "--------------------------------\n",
    "3. ANSWER MODE\n",
    "--------------------------------\n",
    "Decide how the answer should be produced:\n",
    "\n",
    "Use \"direct\" ONLY if:\n",
    "- The question is simple, factual, and well-established\n",
    "- The answer is unlikely to change over time\n",
    "- No citations, verification, or freshness are required\n",
    "\n",
    "Use \"grounded\" if:\n",
    "- The query is research-related\n",
    "- Evidence, papers, or verification are expected\n",
    "- Even if you know the answer, it should be supported by sources\n",
    "\n",
    "Use \"refuse\" if:\n",
    "- The query is not research-related\n",
    "- Or answering would require speculation or unsupported claims\n",
    "\n",
    "Rules:\n",
    "- For research questions, default to \"grounded\" unless clearly trivial\n",
    "- Do NOT choose \"direct\" just because you know the answer\n",
    "- Research assistants prefer evidence over memory\n",
    "\n",
    "--------------------------------\n",
    "4. CONSISTENCY RULES (STRICT)\n",
    "--------------------------------\n",
    "These rules MUST ALWAYS hold:\n",
    "\n",
    "- If research_relevant = false:\n",
    "  - retrieval_mode MUST be \"none\"\n",
    "  - answer_mode MUST be \"refuse\"\n",
    "\n",
    "- If answer_mode = \"direct\":\n",
    "  - retrieval_mode MUST be \"none\"\n",
    "\n",
    "- If answer_mode = \"grounded\":\n",
    "  - retrieval_mode MUST NOT be \"none\"\n",
    "\n",
    "--------------------------------\n",
    "5. CONFIDENCE\n",
    "--------------------------------\n",
    "Provide a confidence score between 0.0 and 1.0 indicating how certain you are that:\n",
    "- research_relevant\n",
    "- retrieval_mode\n",
    "- answer_mode\n",
    "\n",
    "are all correct.\n",
    "\n",
    "Confidence interpretation:\n",
    "- ≥ 0.8 → clear and unambiguous decision\n",
    "- 0.5–0.8 → some uncertainty, retries may be useful\n",
    "- < 0.5 → high uncertainty\n",
    "\n",
    "--------------------------------\n",
    "OUTPUT FORMAT (STRICT)\n",
    "--------------------------------\n",
    "Output ONLY valid JSON that conforms exactly to the provided schema.\n",
    "Do NOT include explanations, comments, or extra text.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt=ChatPromptTemplate([\n",
    "    (\"system\",PLANNER_PROMPT+\"\\n\\n{format_instructions}\"),\n",
    "    (\"user\",'{user_query}')\n",
    "]).partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "chain = prompt | llm |parser\n",
    "\n",
    "response=chain.invoke({\"user_query\":\"who are ypou\"})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b595edb",
   "metadata": {},
   "source": [
    "<h1>RAG INGESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d6edf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = {\n",
    "    \"computer_vision\": \"computer vision\",\n",
    "    \"nlp\": \"natural language processing\",\n",
    "    \"deep_learning\": \"deep learning\",\n",
    "    \"transformers\": \"transformer models\",\n",
    "    \"diffusion\": \"diffusion models\",\n",
    "    \"reinforcement_learning\": \"reinforcement learning\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55e95408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chinn\\AppData\\Local\\Temp\\ipykernel_13916\\2435499132.py:89: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for paper in tqdm(search.results(), desc=f\"Ingesting {topic}\"):\n",
      "Ingesting computer_vision: 2it [00:02,  1.00s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 103\u001b[0m\n\u001b[0;32m    100\u001b[0m meta_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(META_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marxiv_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# ---- SAFE PDF DOWNLOAD\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m downloaded \u001b[38;5;241m=\u001b[39m \u001b[43msafe_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m downloaded:\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[26], line 63\u001b[0m, in \u001b[0;36msafe_download\u001b[1;34m(paper, pdf_path, retries)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries):\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43mpaper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError, urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\chinn\\anaconda3\\envs\\gemini\\Lib\\site-packages\\arxiv\\__init__.py:222\u001b[0m, in \u001b[0;36mResult.download_pdf\u001b[1;34m(self, dirpath, filename, download_domain)\u001b[0m\n\u001b[0;32m    220\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirpath, filename)\n\u001b[0;32m    221\u001b[0m pdf_url \u001b[38;5;241m=\u001b[39m Result\u001b[38;5;241m.\u001b[39m_substitute_domain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpdf_url, download_domain)\n\u001b[1;32m--> 222\u001b[0m written_path, _ \u001b[38;5;241m=\u001b[39m \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m written_path\n",
      "File \u001b[1;32mc:\\Users\\chinn\\anaconda3\\envs\\gemini\\Lib\\urllib\\request.py:268\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reporthook:\n\u001b[0;32m    266\u001b[0m     reporthook(blocknum, bs, size)\n\u001b[1;32m--> 268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m block \u001b[38;5;241m:=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    269\u001b[0m     read \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(block)\n\u001b[0;32m    270\u001b[0m     tfp\u001b[38;5;241m.\u001b[39mwrite(block)\n",
      "File \u001b[1;32mc:\\Users\\chinn\\anaconda3\\envs\\gemini\\Lib\\http\\client.py:472\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    471\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 472\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\chinn\\anaconda3\\envs\\gemini\\Lib\\socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chinn\\anaconda3\\envs\\gemini\\Lib\\ssl.py:1249\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1246\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1247\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1248\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\chinn\\anaconda3\\envs\\gemini\\Lib\\ssl.py:1105\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1106\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1107\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ingest.py\n",
    "import os\n",
    "import json\n",
    "import arxiv\n",
    "import fitz\n",
    "import time\n",
    "import uuid\n",
    "import urllib.error\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "TARGET_PAPER_COUNT = 100\n",
    "RESULTS_PER_TOPIC = 40   # oversample because some will fail\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "PDF_DIR = os.path.join(DATA_DIR, \"pdfs\")\n",
    "META_DIR = os.path.join(DATA_DIR, \"metadata\")\n",
    "\n",
    "COLLECTION_NAME = \"arxiv_ai_research_papers\"\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "TOPICS = {\n",
    "    \"computer_vision\": \"computer vision\",\n",
    "    \"nlp\": \"natural language processing\",\n",
    "    \"deep_learning\": \"deep learning\",\n",
    "    \"transformers\": \"transformer models\",\n",
    "    \"diffusion\": \"diffusion models\",\n",
    "    \"reinforcement_learning\": \"reinforcement learning\"\n",
    "}\n",
    "\n",
    "# ---------------- SETUP ----------------\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "os.makedirs(META_DIR, exist_ok=True)\n",
    "\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "qdrant = QdrantClient(path='./qdrant_data')\n",
    "\n",
    "if COLLECTION_NAME not in [c.name for c in qdrant.get_collections().collections]:\n",
    "    qdrant.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(\n",
    "            size=embedder.get_sentence_embedding_dimension(),\n",
    "            distance=Distance.COSINE\n",
    "        )\n",
    "    )\n",
    "\n",
    "# ---------------- HELPERS ----------------\n",
    "def extract_text(pdf_path: str) -> str:\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        return \"\\n\".join(page.get_text() for page in doc)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def safe_download(paper, pdf_path, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            paper.download_pdf(filename=pdf_path)\n",
    "            return True\n",
    "        except (urllib.error.HTTPError, urllib.error.URLError) as e:\n",
    "            time.sleep(2)\n",
    "        except Exception:\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "# ---------------- INGEST ----------------\n",
    "points = []\n",
    "bm25_corpus = []\n",
    "bm25_ids = []\n",
    "seen_ids = set()\n",
    "\n",
    "successful_papers = 0\n",
    "\n",
    "for topic, query in TOPICS.items():\n",
    "    if successful_papers >= TARGET_PAPER_COUNT:\n",
    "        break\n",
    "\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=RESULTS_PER_TOPIC,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    for paper in tqdm(search.results(), desc=f\"Ingesting {topic}\"):\n",
    "        if successful_papers >= TARGET_PAPER_COUNT:\n",
    "            break\n",
    "\n",
    "        arxiv_id = paper.get_short_id()\n",
    "        if arxiv_id in seen_ids:\n",
    "            continue\n",
    "\n",
    "        seen_ids.add(arxiv_id)\n",
    "\n",
    "        pdf_path = os.path.join(PDF_DIR, f\"{arxiv_id}.pdf\")\n",
    "        meta_path = os.path.join(META_DIR, f\"{arxiv_id}.json\")\n",
    "\n",
    "        # ---- SAFE PDF DOWNLOAD\n",
    "        downloaded = safe_download(paper, pdf_path)\n",
    "        if not downloaded:\n",
    "            continue\n",
    "\n",
    "        # ---- METADATA\n",
    "        metadata = {\n",
    "            \"arxiv_id\": arxiv_id,\n",
    "            \"title\": paper.title,\n",
    "            \"authors\": [a.name for a in paper.authors],\n",
    "            \"summary\": paper.summary,\n",
    "            \"published\": paper.published.isoformat(),\n",
    "            \"updated\": paper.updated.isoformat(),\n",
    "            \"categories\": paper.categories,\n",
    "            \"topic\": topic,\n",
    "            \"pdf_path\": pdf_path,\n",
    "            \"arxiv_url\": paper.entry_id\n",
    "        }\n",
    "\n",
    "        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "\n",
    "        # ---- TEXT EXTRACTION\n",
    "        text = extract_text(pdf_path)\n",
    "        if len(text.strip()) < 500:\n",
    "            continue  # trash PDFs\n",
    "\n",
    "        # ---- EMBEDDING\n",
    "        embedding = embedder.encode(text, normalize_embeddings=True)\n",
    "\n",
    "        tokens = text.lower().split()\n",
    "        bm25_corpus.append(tokens)\n",
    "        bm25_ids.append(arxiv_id)\n",
    "\n",
    "        point_id = uuid.uuid5(\n",
    "            uuid.NAMESPACE_URL,\n",
    "            f\"https://arxiv.org/abs/{arxiv_id}\"\n",
    "        )\n",
    "\n",
    "        points.append(\n",
    "            PointStruct(\n",
    "                id=str(point_id),  # ✅ valid UUID\n",
    "                vector=embedding.tolist(),\n",
    "                payload=metadata  # contains arxiv_id safely\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "        successful_papers += 1\n",
    "\n",
    "# ---------------- STORE ----------------\n",
    "qdrant.upsert(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    points=points\n",
    ")\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"bm25.json\"), \"w\") as f:\n",
    "    json.dump({\"ids\": bm25_ids, \"corpus\": bm25_corpus}, f)\n",
    "\n",
    "print(f\"SUCCESS: Downloaded and indexed {successful_papers} papers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b620177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 RESULTS:\n",
      "2403.04279v1 10.452823910959184\n",
      "2405.03150v2 10.450573979267343\n",
      "2209.00796v15 10.43517542632276\n",
      "2206.01714v6 10.40368595079942\n",
      "2409.07253v3 10.346040968506625\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# ---- Load BM25 data\n",
    "with open(\"data/bm25.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "corpus = data[\"corpus\"]\n",
    "ids = data[\"ids\"]\n",
    "\n",
    "bm25 = BM25Okapi(corpus)\n",
    "\n",
    "# ---- Query\n",
    "query = \"diffusion model image generation\"\n",
    "query_tokens = query.lower().split()\n",
    "\n",
    "scores = bm25.get_scores(query_tokens)\n",
    "top_k = sorted(\n",
    "    zip(ids, scores),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:5]\n",
    "\n",
    "print(\"BM25 RESULTS:\")\n",
    "for arxiv_id, score in top_k:\n",
    "    print(arxiv_id, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07f41484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chinn\\anaconda3\\envs\\gemini\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDRANT RESULTS:\n",
      "2402.05964v2 A Survey on Transformer Compression\n",
      "2302.14017v1 Full Stack Optimization of Transformer Inference: a Survey\n",
      "1901.02860v3 Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\n",
      "2110.08975v2 Deep Transfer Learning & Beyond: Transformer Language Models in Information Systems Research\n",
      "2405.09508v2 Modeling Bilingual Sentence Processing: Evaluating RNN and Transformer Architectures for Cross-Language Structural Priming\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "COLLECTION = \"arxiv_ai_research_papers\"\n",
    "\n",
    "client = QdrantClient(path='./qdrant_data')\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "query = \"transformer architectures for large language models\"\n",
    "query_vector = embedder.encode(query, normalize_embeddings=True).tolist()\n",
    "\n",
    "response = client.query_points(\n",
    "    collection_name=COLLECTION,\n",
    "    query=query_vector,\n",
    "    limit=5,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "print(\"QDRANT RESULTS:\")\n",
    "for p in response.points:\n",
    "    print(p.payload[\"arxiv_id\"], p.payload[\"title\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45925bed",
   "metadata": {},
   "source": [
    "<h1> RAG RETRIEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f52a8067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chinn\\anaconda3\\envs\\gemini\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYBRID RESULTS:\n",
      "2303.02490v2 Diffusion Models Generate Images Like Painters: an Analytical Theory of Outline First, Details Later\n",
      "2209.00796v15 Diffusion Models: A Comprehensive Survey of Methods and Applications\n",
      "2405.03150v2 Video Diffusion Models: A Survey\n",
      "2302.14368v5 Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods\n",
      "2206.01714v6 Compositional Visual Generation with Composable Diffusion Models\n",
      "2403.04279v1 Controllable Generation with Text-to-Image Diffusion Models: A Survey\n",
      "2409.07253v3 Alignment of Diffusion Models: Fundamentals, Challenges, and Future\n",
      "2401.08741v1 Fixed Point Diffusion Models\n",
      "1402.0859v3 The Informed Sampler: A Discriminative Approach to Bayesian Inference in Generative Computer Vision Models\n",
      "2311.03226v1 LDM3D-VR: Latent Diffusion Model for 3D VR\n",
      "1910.13796v1 Deep Learning vs. Traditional Computer Vision\n",
      "2406.00239v1 A Review of Pulse-Coupled Neural Network Applications in Computer Vision and Image Processing\n",
      "2305.16223v2 Prompt-Free Diffusion: Taking \"Text\" out of Text-to-Image Diffusion Models\n",
      "1409.1484v3 The Evolution of First Person Vision Methods: A Survey\n",
      "2209.03855v4 SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion\n",
      "2312.03397v1 Generalized Contrastive Divergence: Joint Training of Energy-Based Model and Diffusion Model through Inverse Reinforcement Learning\n",
      "1611.02145v1 Crowdsourcing in Computer Vision\n",
      "2111.11066v1 FedCV: A Federated Learning Framework for Diverse Computer Vision Tasks\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "COLLECTION = \"arxiv_ai_research_papers\"\n",
    "\n",
    "# ---- Load BM25\n",
    "with open(\"data/bm25.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "bm25 = BM25Okapi(data[\"corpus\"])\n",
    "ids = data[\"ids\"]\n",
    "\n",
    "# ---- Init\n",
    "client = QdrantClient(path='./qdrant_data')\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "query = \"diffusion models for computer vision\"\n",
    "tokens = query.lower().split()\n",
    "\n",
    "# ---- BM25 stage\n",
    "bm25_scores = bm25.get_scores(tokens)\n",
    "bm25_top = sorted(\n",
    "    zip(ids, bm25_scores),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:30]\n",
    "\n",
    "bm25_ids = set(i for i, _ in bm25_top)\n",
    "\n",
    "# ---- Dense stage\n",
    "query_vector = embedder.encode(query, normalize_embeddings=True).tolist()\n",
    "\n",
    "dense = client.query_points(\n",
    "    collection_name=COLLECTION,\n",
    "    query=query_vector,\n",
    "    limit=20,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "print(\"HYBRID RESULTS:\")\n",
    "for p in dense.points:\n",
    "    if p.payload[\"arxiv_id\"] in bm25_ids:\n",
    "        print(p.payload[\"arxiv_id\"], p.payload[\"title\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a82e4",
   "metadata": {},
   "source": [
    "<h1> Websearch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96f8b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de0ca11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What are the latest developments in quantum computing?', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.openaccessgovernment.org/the-latest-developments-in-quantum-computing-a-transformative-frontier/187748/', 'title': 'The latest developments in quantum computing', 'content': 'One major breakthrough involves the development of hypercube network technologies, which enhance the scalability and performance of quantum', 'score': 0.9237645, 'raw_content': None}, {'url': 'https://www.usdsi.org/data-science-insights/latest-developments-in-quantum-computing-2026-edition', 'title': 'Latest Developments in Quantum Computing - 2026 Edition', 'content': 'Latest Developments in Quantum Computing - 2026 Edition · Quantum-as-a-Service (QaaS) and Democratization · Hybrid Classical-Quantum', 'score': 0.84277284, 'raw_content': None}, {'url': 'https://www.mckinsey.com/capabilities/tech-and-ai/our-insights/the-year-of-quantum-from-concept-to-reality-in-2025', 'title': 'The Year of Quantum: From concept to reality in 2025', 'content': 'Explore the latest advancements in quantum computing, sensing, and communication with our comprehensive Quantum Technology Monitor 2025.', 'score': 0.72231215, 'raw_content': None}, {'url': 'https://www.reddit.com/r/QuantumComputing/comments/1chaa7e/current_state_on_quantum_computing_what_are_the/', 'title': 'Current state on quantum computing. What are the major ...', 'content': 'Horizon Computing, Classiq Technologies, Quantum Brilliance, Q-CTRL, OQC, etc. The more specific question you ask is if there is any chance', 'score': 0.7062922, 'raw_content': None}, {'url': 'https://news.mit.edu/topic/quantum-computing', 'title': 'Quantum computing', 'content': 'MIT engineers advance toward a fault-tolerant quantum computer. Researchers achieved a type of coupling between artificial atoms and photons that could enable', 'score': 0.60861784, 'raw_content': None}], 'response_time': 0.86, 'request_id': '118b2544-195b-4917-9db5-aac1f30e7a99'}\n"
     ]
    }
   ],
   "source": [
    "from tavily import TavilyClient\n",
    "client = TavilyClient(os.environ.get(\"TAVILY_API_KEY\"))\n",
    "response = client.search(\n",
    "    query=\"What are the latest developments in quantum computing?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6065cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class RagRetrievalResult(BaseModel):\n",
    "    source: Literal[\"rag\"] = \"rag\"\n",
    "\n",
    "    arxiv_id: str\n",
    "    title: str\n",
    "    abstract: Optional[str]\n",
    "\n",
    "    bm25_score: float\n",
    "    dense_score: float\n",
    "    relevance_score: float\n",
    "\n",
    "    url: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c711411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def rag_retrieve(query: str) -> List[RagRetrievalResult]:\n",
    "    tokens = query.lower().split()\n",
    "\n",
    "    # ---- BM25 stage\n",
    "    bm25_scores = bm25.get_scores(tokens)\n",
    "    bm25_top = sorted(\n",
    "        zip(ids, bm25_scores),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:30]\n",
    "\n",
    "    bm25_map = {i: score for i, score in bm25_top}\n",
    "\n",
    "    # ---- Dense stage\n",
    "    query_vector = embedder.encode(\n",
    "        query,\n",
    "        normalize_embeddings=True\n",
    "    ).tolist()\n",
    "\n",
    "    dense = client.query_points(\n",
    "        collection_name=COLLECTION,\n",
    "        query=query_vector,\n",
    "        limit=20,\n",
    "        with_payload=True,\n",
    "        with_vectors=False\n",
    "    )\n",
    "\n",
    "    results: List[RagRetrievalResult] = []\n",
    "\n",
    "    for p in dense.points:\n",
    "        arxiv_id = p.payload.get(\"arxiv_id\")\n",
    "\n",
    "        # enforce hybrid intersection\n",
    "        if arxiv_id not in bm25_map:\n",
    "            continue\n",
    "\n",
    "        bm25_score = bm25_map[arxiv_id]\n",
    "        dense_score = p.score\n",
    "\n",
    "        # simple but effective fusion\n",
    "        relevance_score = 0.6 * dense_score + 0.4 * min(bm25_score / 10.0, 1.0)\n",
    "\n",
    "        results.append(\n",
    "            RagRetrievalResult(\n",
    "                arxiv_id=arxiv_id,\n",
    "                title=p.payload.get(\"title\", \"\"),\n",
    "                abstract=p.payload.get(\"abstract\"),\n",
    "                bm25_score=bm25_score,\n",
    "                dense_score=dense_score,\n",
    "                relevance_score=relevance_score,\n",
    "                url=f\"https://arxiv.org/abs/{arxiv_id}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # sort by final relevance\n",
    "    results.sort(key=lambda r: r.relevance_score, reverse=True)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "865e64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class WebRetrievalResult(BaseModel):\n",
    "    source: Literal[\"web\"] = \"web\"\n",
    "\n",
    "    title: str\n",
    "    content: str\n",
    "    url: str\n",
    "\n",
    "    relevance_score: float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b1d7422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tavily import TavilyClient\n",
    "\n",
    "tavily = TavilyClient(os.environ.get(\"TAVILY_API_KEY\"))\n",
    "\n",
    "def web_search(query: str, k: int = 8) -> List[WebRetrievalResult]:\n",
    "    response = tavily.search(\n",
    "        query=query,\n",
    "        max_results=k\n",
    "    )\n",
    "\n",
    "    results: List[WebRetrievalResult] = []\n",
    "\n",
    "    for r in response.get(\"results\", []):\n",
    "        results.append(\n",
    "            WebRetrievalResult(\n",
    "                title=r.get(\"title\", \"\"),\n",
    "                content=r.get(\"content\", \"\"),\n",
    "                url=r.get(\"url\"),\n",
    "                relevance_score=float(r.get(\"score\", 0.5))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0e040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
