{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b2807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf85f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e5dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict,List,Dict,Optional,Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetryState(TypedDict):\n",
    "    rag:int\n",
    "    web_search:int\n",
    "    synthesis:int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d49fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalResult(TypedDict):\n",
    "    source: Literal[\"rag\", \"web\"]\n",
    "    title: str\n",
    "    content: str\n",
    "    url: Optional[str]\n",
    "    score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1db98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict,List,Dict,Optional,Literal\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    user_query:str\n",
    "\n",
    "    confidence_score:float\n",
    "    retrieval_mode:Literal['rag','web','both','none']\n",
    "    research_relevant:bool\n",
    "    answer_mode:Literal['grounded','direct','refuse']\n",
    "\n",
    "    rag_results:Optional[List[RetrievalResult]]\n",
    "    web_search_results:Optional[List[RetrievalResult]]\n",
    "    merged_results:Optional[List[RetrievalResult]]\n",
    "\n",
    "    retries:RetryState\n",
    "    max_retries:RetryState\n",
    "\n",
    "    failure_response:Optional[str]\n",
    "    response:Optional[str]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"planner\", planner_node)\n",
    "graph.add_node(\"rag\", rag_node)\n",
    "graph.add_node(\"web_search\", websearch_node)\n",
    "graph.add_node(\"confidence_checker\", confidence_checker)\n",
    "graph.add_node(\"evaluator\", evaluator_node)\n",
    "graph.add_node(\"summarizer\", summarizer_node)\n",
    "graph.add_node(\"failure_node\", failure_node)\n",
    "\n",
    "graph.set_entry_point(\"planner\")\n",
    "\n",
    "# Planner decides retrieval\n",
    "graph.add_conditional_edges(\n",
    "    \"planner\",\n",
    "    planner_router,\n",
    "    {\n",
    "        \"rag\": \"rag\",\n",
    "        \"web\": \"web_search\",\n",
    "        \"both\": [\"rag\", \"web_search\"],\n",
    "        \"none\": \"failure_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Retrieval → confidence\n",
    "graph.add_edge(\"rag\", \"confidence_checker\")\n",
    "graph.add_edge(\"web_search\", \"confidence_checker\")\n",
    "\n",
    "# Confidence → evaluation\n",
    "graph.add_edge(\"confidence_checker\", \"evaluator\")\n",
    "\n",
    "# Evaluation controls loop\n",
    "graph.add_conditional_edges(\n",
    "    \"evaluator\",\n",
    "    evaluation_router,\n",
    "    {\n",
    "        \"planner\": \"planner\",\n",
    "        \"failed\": \"failure_node\",\n",
    "        \"enough\": \"summarizer\"\n",
    "    }\n",
    ")\n",
    "\n",
    "graph.add_edge(\"summarizer\", END)\n",
    "graph.add_edge(\"failure_node\", END)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc0fd8a",
   "metadata": {},
   "source": [
    "<h1>Planner Node</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff428de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "\n",
    "class PlannerOutput(BaseModel):\n",
    "    research_relevant: bool\n",
    "    retrieval_mode: Literal[\"rag\", \"web\", \"both\", \"none\"]\n",
    "    answer_mode: Literal[\"grounded\",\"direct\",\"refuse\"]\n",
    "    confidence: float = Field(ge=0.0, le=1.0)\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=PlannerOutput)\n",
    "\n",
    "\n",
    "PLANNER_PROMPT = \"\"\"You are a planning module inside an AI research assistant.\n",
    "\n",
    "You DO NOT answer the user.\n",
    "You DO NOT explain your reasoning.\n",
    "You ONLY decide how the system should respond.\n",
    "\n",
    "You have access to:\n",
    "- A large internal vector store containing AI / ML / NLP / LLM / Systems research papers, surveys, benchmarks, and technical articles.\n",
    "- A web search tool for retrieving recent, external, or missing research information.\n",
    "\n",
    "Your job is to analyze the user query and output a structured decision describing:\n",
    "1. Whether the query is research-related\n",
    "2. How information should be retrieved (if at all)\n",
    "3. How the answer should be produced\n",
    "4. How confident you are in these decisions\n",
    "\n",
    "--------------------------------\n",
    "1. RESEARCH RELEVANCE\n",
    "--------------------------------\n",
    "A query IS research-related if it involves:\n",
    "- Research papers, surveys, or academic work\n",
    "- Models, algorithms, architectures, or methods\n",
    "- Benchmarks, evaluations, comparisons, or ablations\n",
    "- Technical analysis of AI/ML/NLP/LLMs/Systems\n",
    "\n",
    "A query is NOT research-related if it involves:\n",
    "- Casual conversation or chit-chat\n",
    "- Opinions, jokes, or storytelling\n",
    "- Personal advice or life questions\n",
    "- Shopping, travel, or entertainment\n",
    "- General trivia without technical depth\n",
    "\n",
    "Set:\n",
    "- research_relevant = true or false\n",
    "\n",
    "--------------------------------\n",
    "2. RETRIEVAL MODE\n",
    "--------------------------------\n",
    "If research_relevant = false:\n",
    "- retrieval_mode MUST be \"none\"\n",
    "\n",
    "If research_relevant = true, choose exactly ONE:\n",
    "\n",
    "Use \"rag\" if:\n",
    "- The topic is established or well-documented\n",
    "- Surveys, classic papers, or known methods are sufficient\n",
    "- Internal vector store is likely enough\n",
    "\n",
    "Use \"web\" if:\n",
    "- The query explicitly asks for recent, latest, or current work\n",
    "- The topic involves fast-moving developments\n",
    "- External or up-to-date sources are required\n",
    "\n",
    "Use \"both\" if:\n",
    "- Foundational research exists internally\n",
    "- AND recent updates, comparisons, or new papers may be required\n",
    "\n",
    "Rules:\n",
    "- Do NOT assume freshness unless explicitly requested\n",
    "- Prefer internal knowledge when possible\n",
    "- Be conservative: choose \"both\" only when clearly necessary\n",
    "- Never hallucinate missing information\n",
    "\n",
    "--------------------------------\n",
    "3. ANSWER MODE\n",
    "--------------------------------\n",
    "Decide how the answer should be produced:\n",
    "\n",
    "Use \"direct\" ONLY if:\n",
    "- The question is simple, factual, and well-established\n",
    "- The answer is unlikely to change over time\n",
    "- No citations, verification, or freshness are required\n",
    "\n",
    "Use \"grounded\" if:\n",
    "- The query is research-related\n",
    "- Evidence, papers, or verification are expected\n",
    "- Even if you know the answer, it should be supported by sources\n",
    "\n",
    "Use \"refuse\" if:\n",
    "- The query is not research-related\n",
    "- Or answering would require speculation or unsupported claims\n",
    "\n",
    "Rules:\n",
    "- For research questions, default to \"grounded\" unless clearly trivial\n",
    "- Do NOT choose \"direct\" just because you know the answer\n",
    "- Research assistants prefer evidence over memory\n",
    "\n",
    "--------------------------------\n",
    "4. CONSISTENCY RULES (STRICT)\n",
    "--------------------------------\n",
    "These rules MUST ALWAYS hold:\n",
    "\n",
    "- If research_relevant = false:\n",
    "  - retrieval_mode MUST be \"none\"\n",
    "  - answer_mode MUST be \"refuse\"\n",
    "\n",
    "- If answer_mode = \"direct\":\n",
    "  - retrieval_mode MUST be \"none\"\n",
    "\n",
    "- If answer_mode = \"grounded\":\n",
    "  - retrieval_mode MUST NOT be \"none\"\n",
    "\n",
    "--------------------------------\n",
    "5. CONFIDENCE\n",
    "--------------------------------\n",
    "Provide a confidence score between 0.0 and 1.0 indicating how certain you are that:\n",
    "- research_relevant\n",
    "- retrieval_mode\n",
    "- answer_mode\n",
    "\n",
    "are all correct.\n",
    "\n",
    "Confidence interpretation:\n",
    "- ≥ 0.8 → clear and unambiguous decision\n",
    "- 0.5–0.8 → some uncertainty, retries may be useful\n",
    "- < 0.5 → high uncertainty\n",
    "\n",
    "--------------------------------\n",
    "OUTPUT FORMAT (STRICT)\n",
    "--------------------------------\n",
    "Output ONLY valid JSON that conforms exactly to the provided schema.\n",
    "Do NOT include explanations, comments, or extra text.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt=ChatPromptTemplate([\n",
    "    (\"system\",PLANNER_PROMPT+\"\\n\\n{format_instructions}\"),\n",
    "    (\"user\",'{user_query}')\n",
    "]).partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "chain = prompt | llm |parser\n",
    "\n",
    "response=chain.invoke({\"user_query\":\"who are ypou\"})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b595edb",
   "metadata": {},
   "source": [
    "<h1>RAG INGESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6edf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = {\n",
    "    \"computer_vision\": \"computer vision\",\n",
    "    \"nlp\": \"natural language processing\",\n",
    "    \"deep_learning\": \"deep learning\",\n",
    "    \"transformers\": \"transformer models\",\n",
    "    \"diffusion\": \"diffusion models\",\n",
    "    \"reinforcement_learning\": \"reinforcement learning\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e95408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest.py\n",
    "import os\n",
    "import json\n",
    "import arxiv\n",
    "import fitz\n",
    "import time\n",
    "import uuid\n",
    "import urllib.error\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "TARGET_PAPER_COUNT = 100\n",
    "RESULTS_PER_TOPIC = 40   # oversample because some will fail\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "PDF_DIR = os.path.join(DATA_DIR, \"pdfs\")\n",
    "META_DIR = os.path.join(DATA_DIR, \"metadata\")\n",
    "\n",
    "COLLECTION_NAME = \"arxiv_ai_research_papers\"\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "TOPICS = {\n",
    "    \"computer_vision\": \"computer vision\",\n",
    "    \"nlp\": \"natural language processing\",\n",
    "    \"deep_learning\": \"deep learning\",\n",
    "    \"transformers\": \"transformer models\",\n",
    "    \"diffusion\": \"diffusion models\",\n",
    "    \"reinforcement_learning\": \"reinforcement learning\"\n",
    "}\n",
    "\n",
    "# ---------------- SETUP ----------------\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "os.makedirs(META_DIR, exist_ok=True)\n",
    "\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "qdrant = QdrantClient(path='./qdrant_data')\n",
    "\n",
    "if COLLECTION_NAME not in [c.name for c in qdrant.get_collections().collections]:\n",
    "    qdrant.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(\n",
    "            size=embedder.get_sentence_embedding_dimension(),\n",
    "            distance=Distance.COSINE\n",
    "        )\n",
    "    )\n",
    "\n",
    "# ---------------- HELPERS ----------------\n",
    "def extract_text(pdf_path: str) -> str:\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        return \"\\n\".join(page.get_text() for page in doc)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def safe_download(paper, pdf_path, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            paper.download_pdf(filename=pdf_path)\n",
    "            return True\n",
    "        except (urllib.error.HTTPError, urllib.error.URLError) as e:\n",
    "            time.sleep(2)\n",
    "        except Exception:\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "# ---------------- INGEST ----------------\n",
    "points = []\n",
    "bm25_corpus = []\n",
    "bm25_ids = []\n",
    "seen_ids = set()\n",
    "\n",
    "successful_papers = 0\n",
    "\n",
    "for topic, query in TOPICS.items():\n",
    "    if successful_papers >= TARGET_PAPER_COUNT:\n",
    "        break\n",
    "\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=RESULTS_PER_TOPIC,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    for paper in tqdm(search.results(), desc=f\"Ingesting {topic}\"):\n",
    "        if successful_papers >= TARGET_PAPER_COUNT:\n",
    "            break\n",
    "\n",
    "        arxiv_id = paper.get_short_id()\n",
    "        if arxiv_id in seen_ids:\n",
    "            continue\n",
    "\n",
    "        seen_ids.add(arxiv_id)\n",
    "\n",
    "        pdf_path = os.path.join(PDF_DIR, f\"{arxiv_id}.pdf\")\n",
    "        meta_path = os.path.join(META_DIR, f\"{arxiv_id}.json\")\n",
    "\n",
    "        # ---- SAFE PDF DOWNLOAD\n",
    "        downloaded = safe_download(paper, pdf_path)\n",
    "        if not downloaded:\n",
    "            continue\n",
    "\n",
    "        # ---- METADATA\n",
    "        metadata = {\n",
    "            \"arxiv_id\": arxiv_id,\n",
    "            \"title\": paper.title,\n",
    "            \"authors\": [a.name for a in paper.authors],\n",
    "            \"summary\": paper.summary,\n",
    "            \"published\": paper.published.isoformat(),\n",
    "            \"updated\": paper.updated.isoformat(),\n",
    "            \"categories\": paper.categories,\n",
    "            \"topic\": topic,\n",
    "            \"pdf_path\": pdf_path,\n",
    "            \"arxiv_url\": paper.entry_id\n",
    "        }\n",
    "\n",
    "        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "\n",
    "        # ---- TEXT EXTRACTION\n",
    "        text = extract_text(pdf_path)\n",
    "        if len(text.strip()) < 500:\n",
    "            continue  # trash PDFs\n",
    "\n",
    "        # ---- EMBEDDING\n",
    "        embedding = embedder.encode(text, normalize_embeddings=True)\n",
    "\n",
    "        tokens = text.lower().split()\n",
    "        bm25_corpus.append(tokens)\n",
    "        bm25_ids.append(arxiv_id)\n",
    "\n",
    "        point_id = uuid.uuid5(\n",
    "            uuid.NAMESPACE_URL,\n",
    "            f\"https://arxiv.org/abs/{arxiv_id}\"\n",
    "        )\n",
    "\n",
    "        points.append(\n",
    "            PointStruct(\n",
    "                id=str(point_id),  # ✅ valid UUID\n",
    "                vector=embedding.tolist(),\n",
    "                payload=metadata  # contains arxiv_id safely\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "        successful_papers += 1\n",
    "\n",
    "# ---------------- STORE ----------------\n",
    "qdrant.upsert(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    points=points\n",
    ")\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"bm25.json\"), \"w\") as f:\n",
    "    json.dump({\"ids\": bm25_ids, \"corpus\": bm25_corpus}, f)\n",
    "\n",
    "print(f\"SUCCESS: Downloaded and indexed {successful_papers} papers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b620177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# ---- Load BM25 data\n",
    "with open(\"data/bm25.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "corpus = data[\"corpus\"]\n",
    "ids = data[\"ids\"]\n",
    "\n",
    "bm25 = BM25Okapi(corpus)\n",
    "\n",
    "# ---- Query\n",
    "query = \"diffusion model image generation\"\n",
    "query_tokens = query.lower().split()\n",
    "\n",
    "scores = bm25.get_scores(query_tokens)\n",
    "top_k = sorted(\n",
    "    zip(ids, scores),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:5]\n",
    "\n",
    "print(\"BM25 RESULTS:\")\n",
    "for arxiv_id, score in top_k:\n",
    "    print(arxiv_id, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f41484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "COLLECTION = \"arxiv_ai_research_papers\"\n",
    "\n",
    "client = QdrantClient(path='./qdrant_data')\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "query = \"transformer architectures for large language models\"\n",
    "query_vector = embedder.encode(query, normalize_embeddings=True).tolist()\n",
    "\n",
    "response = client.query_points(\n",
    "    collection_name=COLLECTION,\n",
    "    query=query_vector,\n",
    "    limit=5,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "print(\"QDRANT RESULTS:\")\n",
    "for p in response.points:\n",
    "    print(p.payload[\"arxiv_id\"], p.payload[\"title\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45925bed",
   "metadata": {},
   "source": [
    "<h1> RAG RETRIEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a8067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "COLLECTION = \"arxiv_ai_research_papers\"\n",
    "\n",
    "# ---- Load BM25\n",
    "with open(\"data/bm25.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "bm25 = BM25Okapi(data[\"corpus\"])\n",
    "ids = data[\"ids\"]\n",
    "\n",
    "# ---- Init\n",
    "client = QdrantClient(path='./qdrant_data')\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "query = \"diffusion models for computer vision\"\n",
    "tokens = query.lower().split()\n",
    "\n",
    "# ---- BM25 stage\n",
    "bm25_scores = bm25.get_scores(tokens)\n",
    "bm25_top = sorted(\n",
    "    zip(ids, bm25_scores),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:30]\n",
    "\n",
    "bm25_ids = set(i for i, _ in bm25_top)\n",
    "\n",
    "# ---- Dense stage\n",
    "query_vector = embedder.encode(query, normalize_embeddings=True).tolist()\n",
    "\n",
    "dense = client.query_points(\n",
    "    collection_name=COLLECTION,\n",
    "    query=query_vector,\n",
    "    limit=20,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "print(\"HYBRID RESULTS:\")\n",
    "for p in dense.points:\n",
    "    if p.payload[\"arxiv_id\"] in bm25_ids:\n",
    "        print(p.payload[\"arxiv_id\"], p.payload[\"title\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a82e4",
   "metadata": {},
   "source": [
    "<h1> Websearch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f8b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0ca11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "client = TavilyClient(os.environ.get(\"TAVILY_API_KEY\"))\n",
    "response = client.search(\n",
    "    query=\"What are the latest developments in quantum computing?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6065cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class RagRetrievalResult(BaseModel):\n",
    "    source: Literal[\"rag\"] = \"rag\"\n",
    "\n",
    "    arxiv_id: str\n",
    "    title: str\n",
    "    abstract: Optional[str]\n",
    "\n",
    "    bm25_score: float\n",
    "    dense_score: float\n",
    "    relevance_score: float\n",
    "\n",
    "    url: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c711411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def rag_retrieve(query: str) -> List[RagRetrievalResult]:\n",
    "    tokens = query.lower().split()\n",
    "\n",
    "    # ---- BM25 stage\n",
    "    bm25_scores = bm25.get_scores(tokens)\n",
    "    bm25_top = sorted(\n",
    "        zip(ids, bm25_scores),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:30]\n",
    "\n",
    "    bm25_map = {i: score for i, score in bm25_top}\n",
    "\n",
    "    # ---- Dense stage\n",
    "    query_vector = embedder.encode(\n",
    "        query,\n",
    "        normalize_embeddings=True\n",
    "    ).tolist()\n",
    "\n",
    "    dense = client.query_points(\n",
    "        collection_name=COLLECTION,\n",
    "        query=query_vector,\n",
    "        limit=20,\n",
    "        with_payload=True,\n",
    "        with_vectors=False\n",
    "    )\n",
    "\n",
    "    results: List[RagRetrievalResult] = []\n",
    "\n",
    "    for p in dense.points:\n",
    "        arxiv_id = p.payload.get(\"arxiv_id\")\n",
    "\n",
    "        # enforce hybrid intersection\n",
    "        if arxiv_id not in bm25_map:\n",
    "            continue\n",
    "\n",
    "        bm25_score = bm25_map[arxiv_id]\n",
    "        dense_score = p.score\n",
    "\n",
    "        # simple but effective fusion\n",
    "        relevance_score = 0.6 * dense_score + 0.4 * min(bm25_score / 10.0, 1.0)\n",
    "\n",
    "        results.append(\n",
    "            RagRetrievalResult(\n",
    "                arxiv_id=arxiv_id,\n",
    "                title=p.payload.get(\"title\", \"\"),\n",
    "                abstract=p.payload.get(\"abstract\"),\n",
    "                bm25_score=bm25_score,\n",
    "                dense_score=dense_score,\n",
    "                relevance_score=relevance_score,\n",
    "                url=f\"https://arxiv.org/abs/{arxiv_id}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # sort by final relevance\n",
    "    results.sort(key=lambda r: r.relevance_score, reverse=True)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class WebRetrievalResult(BaseModel):\n",
    "    source: Literal[\"web\"] = \"web\"\n",
    "\n",
    "    title: str\n",
    "    content: str\n",
    "    url: str\n",
    "\n",
    "    relevance_score: float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d7422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tavily import TavilyClient\n",
    "\n",
    "tavily = TavilyClient(os.environ.get(\"TAVILY_API_KEY\"))\n",
    "\n",
    "def web_search(query: str, k: int = 8) -> List[WebRetrievalResult]:\n",
    "    response = tavily.search(\n",
    "        query=query,\n",
    "        max_results=k\n",
    "    )\n",
    "\n",
    "    results: List[WebRetrievalResult] = []\n",
    "\n",
    "    for r in response.get(\"results\", []):\n",
    "        results.append(\n",
    "            WebRetrievalResult(\n",
    "                title=r.get(\"title\", \"\"),\n",
    "                content=r.get(\"content\", \"\"),\n",
    "                url=r.get(\"url\"),\n",
    "                relevance_score=float(r.get(\"score\", 0.5))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0e040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chinn\\anaconda3\\envs\\gemini\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from app.graph.graph import graph\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "initial_state = {\n",
    "    \"user_query\": \"What is gradient descent?\",\n",
    "    \"rag_results\": [],\n",
    "    \"web_search_results\": [],\n",
    "    \"evidence_score\": None,\n",
    "    \"retries\": {\n",
    "        \"rag\": 0,\n",
    "        \"web\": 0,\n",
    "        \"synthesis\": 0\n",
    "    },\n",
    "}\n",
    "\n",
    "result = app.invoke(initial_state)\n",
    "\n",
    "print(result[\"final_answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122627d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01matexit\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqdrant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m close_qdrant_client\n\u001b[1;32m----> 3\u001b[0m app \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241m.\u001b[39mcompile()\n\u001b[0;32m      5\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_query\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me about the recent updates in the architercture of transformers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_results\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \n\u001b[0;32m     21\u001b[0m }\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m     25\u001b[0m     initial_state,\n\u001b[0;32m     26\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m ):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'graph' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import atexit\n",
    "from app.llm.qdrant import close_qdrant_client\n",
    "app = graph.compile()\n",
    "\n",
    "initial_state = {\n",
    "    \"user_query\": \"Tell me about the recent updates in the architercture of transformers\",\n",
    "    \"rag_results\": [],\n",
    "    \"web_search_results\": [],\n",
    "    \"evidence_score\": None,\n",
    "    \"retries\": {\n",
    "        \"rag\": 0,\n",
    "        \"web_search\": 0,\n",
    "        \"synthesis\": 0\n",
    "    },\n",
    "    \"max_retries\": {\n",
    "        \"rag\": 2,\n",
    "        \"web_search\": 2,\n",
    "        \"synthesis\": 2\n",
    "    },\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "for event in app.stream(\n",
    "    initial_state,\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    print(event)\n",
    "\n",
    "atexit.register(close_qdrant_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a12890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chinn\\anaconda3\\envs\\gemini\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tplanner(planner)\n",
      "\trag(rag)\n",
      "\tweb_search(web_search)\n",
      "\tboth_retrieval(both_retrieval)\n",
      "\tmerge(merge)\n",
      "\tevidence_checker(evidence_checker)\n",
      "\tevaluator(evaluator)\n",
      "\treducer(reducer)\n",
      "\tsummarizer(summarizer)\n",
      "\tdegraded_summarizer(degraded_summarizer)\n",
      "\tfailure_node(failure_node)\n",
      "\tdirect_answer(direct_answer)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> planner;\n",
      "\tboth_retrieval --> rag;\n",
      "\tboth_retrieval --> web_search;\n",
      "\tevaluator --> reducer;\n",
      "\tevidence_checker --> evaluator;\n",
      "\tmerge --> evidence_checker;\n",
      "\tplanner -. &nbsp;both&nbsp; .-> both_retrieval;\n",
      "\tplanner -. &nbsp;direct&nbsp; .-> direct_answer;\n",
      "\tplanner -. &nbsp;fail&nbsp; .-> failure_node;\n",
      "\tplanner -.-> rag;\n",
      "\tplanner -. &nbsp;web&nbsp; .-> web_search;\n",
      "\trag --> merge;\n",
      "\treducer -. &nbsp;retry_both&nbsp; .-> both_retrieval;\n",
      "\treducer -. &nbsp;degrade_answer&nbsp; .-> degraded_summarizer;\n",
      "\treducer -. &nbsp;fail&nbsp; .-> failure_node;\n",
      "\treducer -. &nbsp;retry_rag&nbsp; .-> rag;\n",
      "\treducer -. &nbsp;summarize&nbsp; .-> summarizer;\n",
      "\treducer -. &nbsp;retry_web&nbsp; .-> web_search;\n",
      "\tweb_search --> merge;\n",
      "\tdegraded_summarizer --> __end__;\n",
      "\tdirect_answer --> __end__;\n",
      "\tfailure_node --> __end__;\n",
      "\tsummarizer --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from app.graph.graph import graph\n",
    "app = graph.compile()\n",
    "\n",
    "print(app.get_graph().draw_mermaid())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72314f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'planner': {'research_relevant': True, 'retrieval_mode': 'both', 'answer_mode': 'grounded'}}\n",
      "{'both_retrieval': {'user_query': 'Tell me about the recent updates in the architercture of transformers', 'retrieval_mode': 'both', 'answer_mode': 'grounded', 'research_relevant': True, 'rag_results': [], 'web_search_results': [], 'evidence_score': None, 'retries': {'rag': 0, 'web_search': 0, 'synthesis': 0}, 'max_retries': {'rag': 2, 'web_search': 2, 'synthesis': 2}}}\n",
      "{'rag': {'rag_results': [RagRetrievalResult(source_type='rag', source_id='arxiv', arxiv_id='2212.14538v2', title='Transformer in Transformer as Backbone for Deep Reinforcement Learning', url='https://arxiv.org/abs/2212.14538v2', abstract=None, text=None, year=2022, version='v2', bm25_score=14.880436083286108, dense_score=0.4103936795247529, relevance_score=0.6462362077148518), RagRetrievalResult(source_type='rag', source_id='arxiv', arxiv_id='2110.08975v2', title='Deep Transfer Learning & Beyond: Transformer Language Models in Information Systems Research', url='https://arxiv.org/abs/2110.08975v2', abstract=None, text=None, year=2021, version='v2', bm25_score=17.453280432504723, dense_score=0.3155303373584353, relevance_score=0.5893182024150612), RagRetrievalResult(source_type='rag', source_id='arxiv', arxiv_id='2208.03987v4', title='Advancing Plain Vision Transformer Towards Remote Sensing Foundation Model', url='https://arxiv.org/abs/2208.03987v4', abstract=None, text=None, year=2022, version='v4', bm25_score=15.265663872880035, dense_score=0.2993177392862408, relevance_score=0.5795906435717445), RagRetrievalResult(source_type='rag', source_id='arxiv', arxiv_id='2505.11918v1', title='Transformers as Unsupervised Learning Algorithms: A study on Gaussian Mixtures', url='https://arxiv.org/abs/2505.11918v1', abstract=None, text=None, year=2025, version='v1', bm25_score=15.61813762111161, dense_score=0.2711416813782114, relevance_score=0.5626850088269268), RagRetrievalResult(source_type='rag', source_id='arxiv', arxiv_id='2302.14017v1', title='Full Stack Optimization of Transformer Inference: a Survey', url='https://arxiv.org/abs/2302.14017v1', abstract=None, text=None, year=2023, version='v1', bm25_score=15.03240486771883, dense_score=0.258783924552951, relevance_score=0.5552703547317707), RagRetrievalResult(source_type='rag', source_id='arxiv', arxiv_id='2303.00957v1', title='Preference Transformer: Modeling Human Preferences using Transformers for RL', url='https://arxiv.org/abs/2303.00957v1', abstract=None, text=None, year=2023, version='v1', bm25_score=16.46009700688094, dense_score=0.2501373685806922, relevance_score=0.5500824211484153), RagRetrievalResult(source_type='rag', source_id='arxiv', arxiv_id='2107.03844v3', title='A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models', url='https://arxiv.org/abs/2107.03844v3', abstract=None, text=None, year=2021, version='v3', bm25_score=16.7627032755968, dense_score=0.1760297344961112, relevance_score=0.5056178406976668)], 'rag_done': True, 'rag_failed': False}}\n",
      "{'web_search': {'web_search_results': [WebRetrievalResult(source_type='web', source_id='medium.com', title='The Evolution of Transformer Architecture: From 2017 to 2024', url='https://medium.com/@arghya05/the-evolution-of-transformer-architecture-from-2017-to-2024-5a967488e63b', content=\"In this blog, we'll dive into the architectural changes that have shaped the modern Transformer, focusing on their implications for model performance and\", snippet=\"In this blog, we'll dive into the architectural changes that have shaped the modern Transformer, focusing on their implications for model performance and\", published_at=None, retrieved_at='2026-01-02T15:08:27.319572', relevance_score=0.6907474), WebRetrievalResult(source_type='web', source_id='epoch.ai', title='How has DeepSeek improved the Transformer architecture?', url='https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture', content=\"This Gradient Updates issue goes over the major changes that went into DeepSeek's most recent model.\", snippet=\"This Gradient Updates issue goes over the major changes that went into DeepSeek's most recent model.\", published_at=None, retrieved_at='2026-01-02T15:08:27.319572', relevance_score=0.5826579), WebRetrievalResult(source_type='web', source_id='huggingface.co', title='What changed in the Transformer architecture - Hugging Face', url='https://huggingface.co/blog/rishiraj/what-changed-in-the-transformer-architecture', content='Modern Transformers achieve better training stability, handle longer sequences, and make more efficient use of hardware resources.', snippet='Modern Transformers achieve better training stability, handle longer sequences, and make more efficient use of hardware resources.', published_at=None, retrieved_at='2026-01-02T15:08:27.319572', relevance_score=0.5818027), WebRetrievalResult(source_type='web', source_id='www.eventum.ai', title='Three Breakthroughs That Shaped the Modern Transformer ...', url='https://www.eventum.ai/resources/blog/three-breakthroughs-that-shaped-the-modern-transformer-architecture', content='A few refinements to the Transformer architecture have proven to be truly significant, improving model quality, stability, and efficiency.', snippet='A few refinements to the Transformer architecture have proven to be truly significant, improving model quality, stability, and efficiency.', published_at=None, retrieved_at='2026-01-02T15:08:27.319572', relevance_score=0.4024631), WebRetrievalResult(source_type='web', source_id='x.com', title='a lot of AI news at the end of the year', url='https://x.com/Extended_Brain/status/2006890535529947472', content='BREAKING: DeepSeek dropped a core Transformer architecture improvement. A traditional transformer is basically a long stack of blocks,', snippet='BREAKING: DeepSeek dropped a core Transformer architecture improvement. A traditional transformer is basically a long stack of blocks,', published_at=None, retrieved_at='2026-01-02T15:08:27.319572', relevance_score=0.39159262), WebRetrievalResult(source_type='web', source_id='www.youtube.com', title='Transformers Explained: The Architecture Behind All Modern LLMs', url='https://www.youtube.com/watch?v=BJHwFmNWduM', content='... update embedding weights Papers discussed: Attention Is All You ... A New Kind of AI Is Emerging And Its Better Than LLMS? TheAIGRID', snippet='... update embedding weights Papers discussed: Attention Is All You ... A New Kind of AI Is Emerging And Its Better Than LLMS? TheAIGRID', published_at=None, retrieved_at='2026-01-02T15:08:27.319572', relevance_score=0.31000513), WebRetrievalResult(source_type='web', source_id='poloclub.github.io', title='LLM Transformer Model Visually Explained', url='https://poloclub.github.io/transformer-explainer/', content='Transformer is a neural network architecture that has fundamentally changed the approach to Artificial Intelligence.', snippet='Transformer is a neural network architecture that has fundamentally changed the approach to Artificial Intelligence.', published_at=None, retrieved_at='2026-01-02T15:08:27.319572', relevance_score=0.2515654), WebRetrievalResult(source_type='web', source_id='en.wikipedia.org', title='Transformer (deep learning) - Wikipedia', url='https://en.wikipedia.org/wiki/Transformer_(deep_learning)', content='Since 2020, transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. The', snippet='Since 2020, transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. The', published_at=None, retrieved_at='2026-01-02T15:08:27.319572', relevance_score=0.19990121)], 'web_done': True, 'web_failed': False}}\n",
      "{'merge': {'rag_results': [RagRetrievalResult(source_type='rag', source_id='arxiv', arxiv_id='2212.14538v2', title='Transformer in Transformer as Backbone for Deep Reinforcement Learning', url='https://arxiv.org/abs/2212.14538v2', abstract=None, text=None, year=2022, version='v2', bm25_score=14.880436083286108, dense_score=0.4103936795247529, relevance_score=0.6462362077148518), RagRetrievalResult(source_type='rag', source_id='arxiv', arxiv_id='2110.08975v2', title='Deep Transfer Learning & Beyond: Transformer Language Models in Information Systems Research', url='https://arxiv.org/abs/2110.08975v2', abstract=None, text=None, year=2021, version='v2', bm25_score=17.453280432504723, dense_score=0.3155303373584353, relevance_score=0.5893182024150612), RagRetrievalResult(source_type='rag', source_id='arxiv', arxiv_id='2208.03987v4', title='Advancing Plain Vision Transformer Towards Remote Sensing Foundation Model', url='https://arxiv.org/abs/2208.03987v4', abstract=None, text=None, year=2022, version='v4', bm25_score=15.265663872880035, dense_score=0.2993177392862408, relevance_score=0.5795906435717445), RagRetrievalResult(source_type='rag', source_id='arxiv', arxiv_id='2505.11918v1', title='Transformers as Unsupervised Learning Algorithms: A study on Gaussian Mixtures', url='https://arxiv.org/abs/2505.11918v1', abstract=None, text=None, year=2025, version='v1', bm25_score=15.61813762111161, dense_score=0.2711416813782114, relevance_score=0.5626850088269268), RagRetrievalResult(source_type='rag', source_id='arxiv', arxiv_id='2302.14017v1', title='Full Stack Optimization of Transformer Inference: a Survey', url='https://arxiv.org/abs/2302.14017v1', abstract=None, text=None, year=2023, version='v1', bm25_score=15.03240486771883, dense_score=0.258783924552951, relevance_score=0.5552703547317707), RagRetrievalResult(source_type='rag', source_id='arxiv', arxiv_id='2303.00957v1', title='Preference Transformer: Modeling Human Preferences using Transformers for RL', url='https://arxiv.org/abs/2303.00957v1', abstract=None, text=None, year=2023, version='v1', bm25_score=16.46009700688094, dense_score=0.2501373685806922, relevance_score=0.5500824211484153), RagRetrievalResult(source_type='rag', source_id='arxiv', arxiv_id='2107.03844v3', title='A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models', url='https://arxiv.org/abs/2107.03844v3', abstract=None, text=None, year=2021, version='v3', bm25_score=16.7627032755968, dense_score=0.1760297344961112, relevance_score=0.5056178406976668)], 'web_search_results': [WebRetrievalResult(source_type='web', source_id='medium.com', title='The Evolution of Transformer Architecture: From 2017 to 2024', url='https://medium.com/@arghya05/the-evolution-of-transformer-architecture-from-2017-to-2024-5a967488e63b', content=\"In this blog, we'll dive into the architectural changes that have shaped the modern Transformer, focusing on their implications for model performance and\", snippet=\"In this blog, we'll dive into the architectural changes that have shaped the modern Transformer, focusing on their implications for model performance and\", published_at=None, retrieved_at='2026-01-02T15:08:27.319572', relevance_score=0.6907474), WebRetrievalResult(source_type='web', source_id='epoch.ai', title='How has DeepSeek improved the Transformer architecture?', url='https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture', content=\"This Gradient Updates issue goes over the major changes that went into DeepSeek's most recent model.\", snippet=\"This Gradient Updates issue goes over the major changes that went into DeepSeek's most recent model.\", published_at=None, retrieved_at='2026-01-02T15:08:27.319572', relevance_score=0.5826579), WebRetrievalResult(source_type='web', source_id='huggingface.co', title='What changed in the Transformer architecture - Hugging Face', url='https://huggingface.co/blog/rishiraj/what-changed-in-the-transformer-architecture', content='Modern Transformers achieve better training stability, handle longer sequences, and make more efficient use of hardware resources.', snippet='Modern Transformers achieve better training stability, handle longer sequences, and make more efficient use of hardware resources.', published_at=None, retrieved_at='2026-01-02T15:08:27.319572', relevance_score=0.5818027), WebRetrievalResult(source_type='web', source_id='www.eventum.ai', title='Three Breakthroughs That Shaped the Modern Transformer ...', url='https://www.eventum.ai/resources/blog/three-breakthroughs-that-shaped-the-modern-transformer-architecture', content='A few refinements to the Transformer architecture have proven to be truly significant, improving model quality, stability, and efficiency.', snippet='A few refinements to the Transformer architecture have proven to be truly significant, improving model quality, stability, and efficiency.', published_at=None, retrieved_at='2026-01-02T15:08:27.319572', relevance_score=0.4024631), WebRetrievalResult(source_type='web', source_id='x.com', title='a lot of AI news at the end of the year', url='https://x.com/Extended_Brain/status/2006890535529947472', content='BREAKING: DeepSeek dropped a core Transformer architecture improvement. A traditional transformer is basically a long stack of blocks,', snippet='BREAKING: DeepSeek dropped a core Transformer architecture improvement. A traditional transformer is basically a long stack of blocks,', published_at=None, retrieved_at='2026-01-02T15:08:27.319572', relevance_score=0.39159262), WebRetrievalResult(source_type='web', source_id='www.youtube.com', title='Transformers Explained: The Architecture Behind All Modern LLMs', url='https://www.youtube.com/watch?v=BJHwFmNWduM', content='... update embedding weights Papers discussed: Attention Is All You ... A New Kind of AI Is Emerging And Its Better Than LLMS? TheAIGRID', snippet='... update embedding weights Papers discussed: Attention Is All You ... A New Kind of AI Is Emerging And Its Better Than LLMS? TheAIGRID', published_at=None, retrieved_at='2026-01-02T15:08:27.319572', relevance_score=0.31000513), WebRetrievalResult(source_type='web', source_id='poloclub.github.io', title='LLM Transformer Model Visually Explained', url='https://poloclub.github.io/transformer-explainer/', content='Transformer is a neural network architecture that has fundamentally changed the approach to Artificial Intelligence.', snippet='Transformer is a neural network architecture that has fundamentally changed the approach to Artificial Intelligence.', published_at=None, retrieved_at='2026-01-02T15:08:27.319572', relevance_score=0.2515654), WebRetrievalResult(source_type='web', source_id='en.wikipedia.org', title='Transformer (deep learning) - Wikipedia', url='https://en.wikipedia.org/wiki/Transformer_(deep_learning)', content='Since 2020, transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. The', snippet='Since 2020, transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. The', published_at=None, retrieved_at='2026-01-02T15:08:27.319572', relevance_score=0.19990121)]}}\n",
      "Evidence checker\n",
      "{'evidence_checker': {'evidence_score': EvidenceScore(coverage=1.0, consistency=1.0, freshness=0.6, source_diversity=9, usable=True)}}\n",
      "{'evaluator': {'next_action': 'summarize'}}\n",
      "{'reducer': {'retries': {'rag': 0, 'web_search': 0, 'synthesis': 0}, 'retrieval_mode': 'both'}}\n",
      "{'summarizer': {'final_response': 'Ypu have reached summarixer node'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function app.llm.qdrant.close_qdrant_client()>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import atexit\n",
    "from app.llm.qdrant import close_qdrant_client\n",
    "\n",
    "initial_state = {\n",
    "    \"user_query\": \"Tell me about the recent updates in the architercture of transformers\",\n",
    "    \"rag_results\": [],\n",
    "    \"web_search_results\": [],\n",
    "    \"evidence_score\": None,\n",
    "    \"retries\": {\n",
    "        \"rag\": 0,\n",
    "        \"web_search\": 0,\n",
    "        \"synthesis\": 0\n",
    "    },\n",
    "    \"max_retries\": {\n",
    "        \"rag\": 2,\n",
    "        \"web_search\": 2,\n",
    "        \"synthesis\": 2\n",
    "    },\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "for event in app.stream(\n",
    "    initial_state,\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    print(event)\n",
    "\n",
    "atexit.register(close_qdrant_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bfceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beea377",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ANSWER MODE:\", result[\"answer_mode\"])\n",
    "print(\"RETRIEVAL MODE:\", result[\"retrieval_mode\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e0335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['web_search_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f21710",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['rag_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a32d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.tools.web_search import web_search\n",
    "web_search(\"recent upgrades in transformer architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef44b48d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
