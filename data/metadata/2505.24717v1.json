{
  "arxiv_id": "2505.24717v1",
  "title": "PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations",
  "authors": [
    "Benjamin Holzschuh",
    "Qiang Liu",
    "Georg Kohl",
    "Nils Thuerey"
  ],
  "summary": "We introduce PDE-Transformer, an improved transformer-based architecture for surrogate modeling of physics simulations on regular grids. We combine recent architectural improvements of diffusion transformers with adjustments specific for large-scale simulations to yield a more scalable and versatile general-purpose transformer architecture, which can be used as the backbone for building large-scale foundation models in physical sciences. We demonstrate that our proposed architecture outperforms state-of-the-art transformer architectures for computer vision on a large dataset of 16 different types of PDEs. We propose to embed different physical channels individually as spatio-temporal tokens, which interact via channel-wise self-attention. This helps to maintain a consistent information density of tokens when learning multiple types of PDEs simultaneously. We demonstrate that our pre-trained models achieve improved performance on several challenging downstream tasks compared to training from scratch and also beat other foundation model architectures for physics simulations.",
  "published": "2025-05-30T15:39:54+00:00",
  "updated": "2025-05-30T15:39:54+00:00",
  "categories": [
    "cs.LG"
  ],
  "topic": "transformers",
  "pdf_path": "data\\pdfs\\2505.24717v1.pdf",
  "arxiv_url": "http://arxiv.org/abs/2505.24717v1"
}