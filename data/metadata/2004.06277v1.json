{
  "arxiv_id": "2004.06277v1",
  "title": "A Demonstration of Issues with Value-Based Multiobjective Reinforcement Learning Under Stochastic State Transitions",
  "authors": [
    "Peter Vamplew",
    "Cameron Foale",
    "Richard Dazeley"
  ],
  "summary": "We report a previously unidentified issue with model-free, value-based approaches to multiobjective reinforcement learning in the context of environments with stochastic state transitions. An example multiobjective Markov Decision Process (MOMDP) is used to demonstrate that under such conditions these approaches may be unable to discover the policy which maximises the Scalarised Expected Return, and in fact may converge to a Pareto-dominated solution. We discuss several alternative methods which may be more suitable for maximising SER in MOMDPs with stochastic transitions.",
  "published": "2020-04-14T02:55:12+00:00",
  "updated": "2020-04-14T02:55:12+00:00",
  "categories": [
    "cs.LG",
    "cs.MA",
    "stat.ML"
  ],
  "topic": "reinforcement_learning",
  "pdf_path": "data\\pdfs\\2004.06277v1.pdf",
  "arxiv_url": "http://arxiv.org/abs/2004.06277v1"
}