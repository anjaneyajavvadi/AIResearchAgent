{
  "arxiv_id": "2107.09957v2",
  "title": "Memorization in Deep Neural Networks: Does the Loss Function matter?",
  "authors": [
    "Deep Patel",
    "P. S. Sastry"
  ],
  "summary": "Deep Neural Networks, often owing to the overparameterization, are shown to be capable of exactly memorizing even randomly labelled data. Empirical studies have also shown that none of the standard regularization techniques mitigate such overfitting. We investigate whether the choice of the loss function can affect this memorization. We empirically show, with benchmark data sets MNIST and CIFAR-10, that a symmetric loss function, as opposed to either cross-entropy or squared error loss, results in significant improvement in the ability of the network to resist such overfitting. We then provide a formal definition for robustness to memorization and provide a theoretical explanation as to why the symmetric losses provide this robustness. Our results clearly bring out the role loss functions alone can play in this phenomenon of memorization.",
  "published": "2021-07-21T09:08:51+00:00",
  "updated": "2021-07-22T05:36:24+00:00",
  "categories": [
    "cs.LG",
    "cs.CV",
    "stat.ML"
  ],
  "topic": "deep_learning",
  "pdf_path": "data\\pdfs\\2107.09957v2.pdf",
  "arxiv_url": "http://arxiv.org/abs/2107.09957v2"
}