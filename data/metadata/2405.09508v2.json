{
  "arxiv_id": "2405.09508v2",
  "title": "Modeling Bilingual Sentence Processing: Evaluating RNN and Transformer Architectures for Cross-Language Structural Priming",
  "authors": [
    "Demi Zhang",
    "Bushi Xiao",
    "Chao Gao",
    "Sangpil Youm",
    "Bonnie J Dorr"
  ],
  "summary": "This study evaluates the performance of Recurrent Neural Network (RNN) and Transformer models in replicating cross-language structural priming, a key indicator of abstract grammatical representations in human language processing. Focusing on Chinese-English priming, which involves two typologically distinct languages, we examine how these models handle the robust phenomenon of structural priming, where exposure to a particular sentence structure increases the likelihood of selecting a similar structure subsequently. Our findings indicate that transformers outperform RNNs in generating primed sentence structures, with accuracy rates that exceed 25.84\\% to 33. 33\\%. This challenges the conventional belief that human sentence processing primarily involves recurrent and immediate processing and suggests a role for cue-based retrieval mechanisms. This work contributes to our understanding of how computational models may reflect human cognitive processes across diverse language families.",
  "published": "2024-05-15T17:01:02+00:00",
  "updated": "2024-10-15T20:24:00+00:00",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "topic": "transformers",
  "pdf_path": "data\\pdfs\\2405.09508v2.pdf",
  "arxiv_url": "http://arxiv.org/abs/2405.09508v2"
}